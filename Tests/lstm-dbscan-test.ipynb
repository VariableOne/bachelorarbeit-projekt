{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6abdb9",
   "metadata": {},
   "source": [
    "# Ein Hybridmodell mit LSTM-AE, DBSCAN und IF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8183868f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 16:53:19.397293: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752504799.416896  239140 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752504799.422886  239140 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752504799.440796  239140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752504799.440833  239140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752504799.440835  239140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752504799.440838  239140 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-14 16:53:19.446809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "import logging\n",
    "from tensorflow.keras.layers import TimeDistributed, Dense, LSTM, Dropout, LayerNormalization, Input, Add, Bidirectional, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import sys\n",
    "from keras.layers import LSTM, Dropout, LayerNormalization, Add, TimeDistributed, Dense\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b37113",
   "metadata": {},
   "source": [
    "## 1. Vorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6d26b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: []\n",
      "Keine GPU verfügbar, läuft auf CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1752504801.939432  239140 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Liste aller GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPUs:\", gpus)\n",
    "\n",
    "if gpus:\n",
    "    # Erzwinge, dass TensorFlow auf GPU 0 rechnet\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.constant([[1.0, 2.0]])\n",
    "        b = tf.constant([[3.0], [4.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        print(\"Result auf GPU:\", c.numpy())\n",
    "else:\n",
    "    print(\"Keine GPU verfügbar, läuft auf CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185398f1",
   "metadata": {},
   "source": [
    "#### Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb5a6c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp log_level             category realmId realmName  \\\n",
      "0 2025-07-12 01:20:55+02:00      info  org.keycloak.events   bosch     bosch   \n",
      "1 2025-07-12 01:48:00+02:00      warn  org.keycloak.events  master    master   \n",
      "2 2025-07-12 02:05:02+02:00      warn  org.keycloak.events  master    master   \n",
      "3 2025-07-12 02:08:17+02:00      warn  org.keycloak.events  master    master   \n",
      "4 2025-07-12 02:31:44+02:00      warn  org.keycloak.events  master    master   \n",
      "\n",
      "    clientId userId     sessionId        ipAddress      authMethod  \\\n",
      "0     broker   Yara  session_3504   198.143.36.129             otp   \n",
      "1  login-app    NaN  session_7099   198.112.119.68        password   \n",
      "2  login-app    NaN  session_7806  203.207.106.189  openid-connect   \n",
      "3  login-app    NaN  session_1824     198.17.0.136            saml   \n",
      "4  login-app    NaN  session_7726    198.2.114.248            code   \n",
      "\n",
      "  resourceType operationType  label         type                error details  \n",
      "0       groups           GET      1          NaN                  NaN     NaN  \n",
      "1          NaN           NaN      1  LOGIN_ERROR  invalid_credentials     NaN  \n",
      "2          NaN           NaN      1  LOGIN_ERROR  invalid_credentials     NaN  \n",
      "3          NaN           NaN      1  LOGIN_ERROR  invalid_credentials     NaN  \n",
      "4          NaN           NaN      1  LOGIN_ERROR  invalid_credentials     NaN  \n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_json(\"train_logs.json\", lines=False)\n",
    "data_test = pd.read_json(\"test_logs.json\", lines=False)\n",
    "data_val = pd.read_json(\"val_logs.json\", lines=False)\n",
    "print(data_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132d010",
   "metadata": {},
   "source": [
    "#### NaN-Werte bereinigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "321cb5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = pd.DataFrame(data_train)\n",
    "X_test_full = pd.DataFrame(data_test)\n",
    "X_val_full = pd.DataFrame(data_val)\n",
    "\n",
    "y_test_full = data_test[\"label\"]\n",
    "y_val_full = data_val[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d90c1e",
   "metadata": {},
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea8dd938",
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = open(\"output_log.txt\", \"w\")\n",
    "logging.basicConfig(stream=logfile, level=logging.INFO)\n",
    "\n",
    "sys.stdout = logfile\n",
    "sys.stderr = logfile\n",
    "\n",
    "logging.info(\"Modelle wurden gestartet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b904244",
   "metadata": {},
   "source": [
    "#### Automatisch alles in Numerische oder Kategorische Daten einteilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeaa7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def is_missing(val):\n",
    "    return val is None or (isinstance(val, float) and np.isnan(val))\n",
    "\n",
    "def auto_encode_features(logs, one_hot_numeric=False, \n",
    "                         label_encoders=None, onehot_encoders=None, fit=True):\n",
    "    if logs is None or len(logs) == 0:\n",
    "        return [], {}, {}\n",
    "\n",
    "    if hasattr(logs, \"to_dict\"):\n",
    "        logs = logs.to_dict(orient='records')\n",
    "\n",
    "    if label_encoders is None:\n",
    "        label_encoders = {}\n",
    "    if onehot_encoders is None:\n",
    "        onehot_encoders = {}\n",
    "\n",
    "    sample = logs[0]\n",
    "    all_keys = sample.keys()\n",
    "    encoded_logs = []\n",
    "\n",
    "    # Alle möglichen Rollen sammeln (optional)\n",
    "    all_possible_roles = set()\n",
    "    if fit:\n",
    "        for log in logs:\n",
    "            roles = log.get(\"roles\", [])\n",
    "            # Fix: Sicherstellen, dass roles eine Liste ist\n",
    "            if not isinstance(roles, (list, tuple)):\n",
    "                if is_missing(roles):\n",
    "                    roles = []\n",
    "                else:\n",
    "                    roles = [roles]\n",
    "\n",
    "            for role in roles:\n",
    "                if is_missing(role):\n",
    "                    role = \"__MISSING__\"\n",
    "                all_possible_roles.add(role)\n",
    "\n",
    "    for log in logs:\n",
    "        encoded = {}\n",
    "        for key in all_keys:\n",
    "            val = log.get(key)\n",
    "\n",
    "            # 🔹 Spezialfall: roles\n",
    "            if key == \"roles\":\n",
    "                roles = val\n",
    "                # Fix: Sicherstellen, dass roles eine Liste ist\n",
    "                if not isinstance(roles, (list, tuple)):\n",
    "                    if is_missing(roles):\n",
    "                        roles = []\n",
    "                    else:\n",
    "                        roles = [roles]\n",
    "\n",
    "                present_roles = set()\n",
    "                for role in roles:\n",
    "                    if is_missing(role):\n",
    "                        role = \"__MISSING__\"\n",
    "                    present_roles.add(role)\n",
    "\n",
    "                # Ein Feature pro Rolle (1 falls vorhanden, sonst 0)\n",
    "                for role in all_possible_roles:\n",
    "                    feature_name = f\"role_{role}\"\n",
    "                    encoded[feature_name] = 1 if role in present_roles else 0\n",
    "\n",
    "                continue  # skip default logic for \"roles\"\n",
    "\n",
    "            # 🔸 Fehlende Werte\n",
    "            if is_missing(val):\n",
    "                val = \"__MISSING__\"\n",
    "\n",
    "            # 🔸 Numerisch\n",
    "            if isinstance(val, (int, float)) and not isinstance(val, bool):\n",
    "                if val == \"__MISSING__\":\n",
    "                    encoded[key] = -9999\n",
    "                elif one_hot_numeric:\n",
    "                    if key not in onehot_encoders and fit:\n",
    "                        values = np.array([[l.get(key) if not is_missing(l.get(key)) else -9999]\n",
    "                                           for l in logs])\n",
    "                        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "                        encoder.fit(values)\n",
    "                        onehot_encoders[key] = encoder\n",
    "\n",
    "                    if key in onehot_encoders:\n",
    "                        enc = onehot_encoders[key].transform([[val]])[0]\n",
    "                        for i, v in enumerate(enc):\n",
    "                            encoded[f\"{key}_{i}\"] = v\n",
    "                else:\n",
    "                    encoded[key] = val\n",
    "\n",
    "            # 🔸 String\n",
    "            elif isinstance(val, str):\n",
    "                if key not in label_encoders and fit:\n",
    "                    values = list(set(\n",
    "                        tuple(l.get(key)) if isinstance(l.get(key), list) \n",
    "                        else (l.get(key) if not is_missing(l.get(key)) else \"__MISSING__\")\n",
    "                        for l in logs\n",
    "                    ))\n",
    "                    le = LabelEncoder()\n",
    "                    le.fit(values)\n",
    "                    label_encoders[key] = le\n",
    "\n",
    "                if key in label_encoders:\n",
    "                    le = label_encoders[key]\n",
    "                    if val in le.classes_:\n",
    "                        encoded[key] = le.transform([val])[0]\n",
    "                    else:\n",
    "                        encoded[key] = -1\n",
    "\n",
    "            # 🔸 Liste (aber nicht \"roles\")\n",
    "            elif isinstance(val, str):\n",
    "                if key not in label_encoders and fit:\n",
    "                    def make_hashable(v):\n",
    "                        if isinstance(v, list):\n",
    "                            return tuple(make_hashable(x) for x in v)\n",
    "                        return v\n",
    "                    \n",
    "                    values = list(set(\n",
    "                        make_hashable(l.get(key)) if not is_missing(l.get(key)) else \"__MISSING__\"\n",
    "                        for l in logs\n",
    "                    ))\n",
    "                    le = LabelEncoder()\n",
    "                    le.fit(values)\n",
    "                    label_encoders[key] = le\n",
    "\n",
    "                if key in label_encoders:\n",
    "                    le = label_encoders[key]\n",
    "                    if val in le.classes_:\n",
    "                        encoded[key] = le.transform([val])[0]\n",
    "                    else:\n",
    "                        encoded[key] = -1\n",
    "\n",
    "        encoded_logs.append(encoded)\n",
    "\n",
    "    return encoded_logs, label_encoders, onehot_encoders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3ff107",
   "metadata": {},
   "source": [
    "#### Werte aus vorheriger Funktion vereinigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4d0002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicts_to_feature_matrix(encoded_logs):\n",
    "    feature_names = sorted({key for d in encoded_logs for key in d.keys()})\n",
    "\n",
    "    X = np.zeros((len(encoded_logs), len(feature_names)), dtype=np.float32)\n",
    "\n",
    "    for i, d in enumerate(encoded_logs):\n",
    "        for j, feat in enumerate(feature_names):\n",
    "            if feat in d:\n",
    "                X[i, j] = d[feat]\n",
    "\n",
    "    return X, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d825cad",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      5\u001b[39m encoded_logs, label_encoders, onehot_encoders = auto_encode_features(\n\u001b[32m      6\u001b[39m     logs_train, one_hot_numeric=\u001b[38;5;28;01mTrue\u001b[39;00m, fit=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m X_train, feature_names = dicts_to_feature_matrix(encoded_logs)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m encoded_test_logs, _, _ = \u001b[43mauto_encode_features\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogs_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_encoders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel_encoders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monehot_encoders\u001b[49m\u001b[43m=\u001b[49m\u001b[43monehot_encoders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     13\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m X_test, _ = dicts_to_feature_matrix(encoded_test_logs)\n\u001b[32m     16\u001b[39m encoded_val_logs, _, _ = auto_encode_features(\n\u001b[32m     17\u001b[39m     logs_val, one_hot_numeric=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     18\u001b[39m     label_encoders=label_encoders, onehot_encoders=onehot_encoders, fit=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mauto_encode_features\u001b[39m\u001b[34m(logs, one_hot_numeric, label_encoders, onehot_encoders, fit)\u001b[39m\n\u001b[32m    105\u001b[39m le = label_encoders[key]\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m le.classes_:\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     encoded[key] = \u001b[43mle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    109\u001b[39m     encoded[key] = -\u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bachelorarbeit-projekt/env/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:134\u001b[39m, in \u001b[36mLabelEncoder.transform\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) == \u001b[32m0\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([])\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bachelorarbeit-projekt/env/lib/python3.12/site-packages/sklearn/utils/_encode.py:235\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp.isdtype(values.dtype, \u001b[33m\"\u001b[39m\u001b[33mnumeric\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/bachelorarbeit-projekt/env/lib/python3.12/site-packages/sklearn/utils/_encode.py:173\u001b[39m, in \u001b[36m_map_to_integer\u001b[39m\u001b[34m(values, uniques)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Map values based on its position in uniques.\"\"\"\u001b[39;00m\n\u001b[32m    172\u001b[39m xp, _ = get_namespace(values, uniques)\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m table = _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device=device(values))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "logs_train = X_train_full.to_dict(orient='records')\n",
    "logs_test = X_test_full.to_dict(orient='records')\n",
    "logs_val = X_val_full.to_dict(orient='records')\n",
    "\n",
    "encoded_logs, label_encoders, onehot_encoders = auto_encode_features(\n",
    "    logs_train, one_hot_numeric=True, fit=True\n",
    ")\n",
    "X_train, feature_names = dicts_to_feature_matrix(encoded_logs)\n",
    "\n",
    "encoded_test_logs, _, _ = auto_encode_features(\n",
    "    logs_test, one_hot_numeric=True,\n",
    "    label_encoders=label_encoders, onehot_encoders=onehot_encoders, fit=False\n",
    ")\n",
    "X_test, _ = dicts_to_feature_matrix(encoded_test_logs)\n",
    "\n",
    "encoded_val_logs, _, _ = auto_encode_features(\n",
    "    logs_val, one_hot_numeric=True,\n",
    "    label_encoders=label_encoders, onehot_encoders=onehot_encoders, fit=False\n",
    ")\n",
    "X_val, _ = dicts_to_feature_matrix(encoded_val_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b55383",
   "metadata": {},
   "source": [
    "#### Skalierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3445ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f8c906",
   "metadata": {},
   "source": [
    "#### PCA zur Dimensionsreduktion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f955d4",
   "metadata": {},
   "source": [
    "### Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 15\n",
    "batch_size = 128\n",
    "\n",
    "encoder_layers = [1024, 512, 256]\n",
    "decoder_layers = [256, 512, 1024]\n",
    "\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31049ba2",
   "metadata": {},
   "source": [
    "### Sequenzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceToSequenceGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, seq_length, batch_size, pad_last=True):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.pad_last = pad_last\n",
    "\n",
    "        self.indices = np.arange(len(data) - seq_length)\n",
    "        \n",
    "        # Wenn Padding aktiv ist und etwas übrig bleibt\n",
    "        if pad_last and (len(data) % seq_length != 0):\n",
    "            self.include_last = True\n",
    "        else:\n",
    "            self.include_last = False\n",
    "\n",
    "    def __len__(self):\n",
    "        base = (len(self.indices) + self.batch_size - 1) // self.batch_size\n",
    "        return base + (1 if self.include_last else 0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self) - 1 or not self.include_last:\n",
    "            batch_idx = self.indices[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            X_batch = np.array([self.data[i:i + self.seq_length] for i in batch_idx])\n",
    "        else:\n",
    "            # letzte Sequenz mit Padding\n",
    "            last_seq = self.data[-self.seq_length:]\n",
    "            if len(last_seq) < self.seq_length:\n",
    "                padding_len = self.seq_length - len(last_seq)\n",
    "                padding = np.zeros((padding_len, self.data.shape[1]))\n",
    "                last_seq = np.vstack((last_seq, padding))\n",
    "            X_batch = np.expand_dims(last_seq, axis=0)\n",
    "\n",
    "        return X_batch, X_batch\n",
    "\n",
    "\n",
    "train_gen = SequenceToSequenceGenerator(X_train_scaled, seq_length, batch_size)\n",
    "val_gen = SequenceToSequenceGenerator(X_val_scaled, seq_length, batch_size)\n",
    "test_gen = SequenceToSequenceGenerator(X_test_scaled, seq_length, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33d7452",
   "metadata": {},
   "source": [
    "## 2. LSTM-AE implementieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8edbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LSTM-Block\n",
    "def residual_lstm_block(inputs, units, dropout_rate=dropout_rate):\n",
    "    shortcut = inputs\n",
    "    x = LSTM(units, return_sequences=True)(inputs)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Falls Dimension nicht passt, anpassen\n",
    "    if K.int_shape(shortcut)[-1] != units:\n",
    "        shortcut = TimeDistributed(Dense(units))(shortcut)\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "n_features = X_train_scaled.shape[1]\n",
    "\n",
    "inputs = Input(shape=(seq_length, n_features))\n",
    "x = inputs\n",
    "\n",
    "# Encoder mit Residual-Blöcken\n",
    "for units in encoder_layers:\n",
    "    x = residual_lstm_block(x, units, dropout_rate)\n",
    "\n",
    "# Bottleneck\n",
    "encoded = Bidirectional(LSTM(encoder_layers[-1]))(x)\n",
    "\n",
    "# Decoder\n",
    "decoded = RepeatVector(seq_length)(encoded)\n",
    "x = decoded\n",
    "\n",
    "for units in decoder_layers:\n",
    "    x = residual_lstm_block(x, units, dropout_rate) \n",
    "\n",
    "outputs = TimeDistributed(Dense(n_features, activation='linear'))(x)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer=Adam(learning_rate), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=5, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ebaae",
   "metadata": {},
   "source": [
    "### Umwandlung der Daten für nächste Modelle: Rekonstruktionsfehler berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ede0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstruction_errors(gen, model):\n",
    "    errors = []\n",
    "    for i in range(len(gen)):\n",
    "        X_batch, _ = gen[i]\n",
    "        pred = model.predict(X_batch, verbose=0)\n",
    "        batch_errors = np.mean(np.square(X_batch - pred), axis=(1, 2))\n",
    "        errors.extend(batch_errors)\n",
    "    return np.array(errors)\n",
    "\n",
    "train_errors = get_reconstruction_errors(train_gen, model)\n",
    "val_errors = get_reconstruction_errors(val_gen, model)\n",
    "test_errors = get_reconstruction_errors(test_gen, model)\n",
    "\n",
    "reconstruction_errors = get_reconstruction_errors(test_gen, model)\n",
    "threshold = np.percentile(train_errors, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00451659",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_seq = np.array([y_test_full[i + seq_length - 1] for i in range(len(reconstruction_errors))])\n",
    "true_labels = y_test_seq.astype(int)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(true_labels, reconstruction_errors)\n",
    "optimal_idx = (tpr - fpr).argmax()\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "y_pred = (reconstruction_errors > optimal_threshold).astype(int)\n",
    "\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, y_pred, average='binary')\n",
    "roc_auc = roc_auc_score(true_labels, reconstruction_errors)\n",
    "logging.info(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1-Score: {f1:.3f}, ROC-AUC: {roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7452d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_rec = StandardScaler()\n",
    "scaler_rec.fit(train_errors.reshape(-1, 1))\n",
    "val_errors_scaled = scaler_rec.transform(val_errors.reshape(-1, 1))\n",
    "test_errors_scaled = scaler_rec.transform(test_errors.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1734c09",
   "metadata": {},
   "source": [
    "# Modell 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2c9f11",
   "metadata": {},
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iforest = IsolationForest()\n",
    "iforest.fit(train_errors.reshape(-1, 1))\n",
    "\n",
    "test_preds_if = iforest.predict(test_errors.reshape(-1, 1))\n",
    "val_anomaly_if = (test_preds_if == -1).astype(int)\n",
    "\n",
    "y_test_seq_if = np.array([y_test_full[i + seq_length- 1] for i in range(len(val_anomaly_if))])\n",
    "true_labels_if = y_test_seq_if.astype(int)\n",
    "\n",
    "report_iforest_if = classification_report(true_labels_if, val_anomaly_if)\n",
    "logging.info(\"Isolation Forest - Test Report:\\n\" + report_iforest_if)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fa46b4",
   "metadata": {},
   "source": [
    "### IF MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_if = matthews_corrcoef(true_labels_if, val_anomaly_if)\n",
    "logging.info(\"Matthews Correlation Coefficient: %f\", mcc_if)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c30aad",
   "metadata": {},
   "source": [
    "### IF Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_acc = balanced_accuracy_score(true_labels_if, val_anomaly_if)\n",
    "logging.info(\"Balanced Accuracy: %f\", balanced_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136e11a",
   "metadata": {},
   "source": [
    "### IF AUC-Kurven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores = -iforest.decision_function(val_errors.reshape(-1,1))\n",
    "y_val_seq = np.array([y_val_full[i + seq_length - 1] for i in range(len(val_scores))])\n",
    "true_labels = y_val_seq.astype(int)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(true_labels, val_scores)\n",
    "roc_auc = roc_auc_score(true_labels, val_scores)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(true_labels, val_scores)\n",
    "pr_auc = average_precision_score(true_labels, val_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add85360",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ROC Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curve_if.png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89f814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f\"AP = {pr_auc:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('pr_curve_if.png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142607f1",
   "metadata": {},
   "source": [
    "# Modell 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c53e25c",
   "metadata": {},
   "source": [
    "## One-Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9085760",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocsvm = OneClassSVM(nu=0.005, gamma=50)\n",
    "ocsvm.fit(train_errors.reshape(-1, 1))\n",
    "\n",
    "test_preds_ocsvm = ocsvm.predict(test_errors.reshape(-1, 1))\n",
    "test_anomaly_ocsvm = (test_preds_ocsvm == -1).astype(int)\n",
    "\n",
    "y_test_seq_ocsvm = np.array([y_test_full[i + seq_length - 1] for i in range(len(test_anomaly_ocsvm))])\n",
    "true_labels_ocsvm = y_test_seq.astype(int)\n",
    "\n",
    "report_ocsvm = classification_report(true_labels_ocsvm, test_anomaly_ocsvm)\n",
    "logging.info(\"One-Class SVM - Test Report:\\n\" + report_ocsvm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296736bf",
   "metadata": {},
   "source": [
    "### OCSVM MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a8958",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = matthews_corrcoef(true_labels_ocsvm, test_anomaly_ocsvm)\n",
    "logging.info(\"Matthews Correlation Coefficient: %f\", mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df4624",
   "metadata": {},
   "source": [
    "### OCSVM Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f42ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_acc = balanced_accuracy_score(true_labels_ocsvm, test_anomaly_ocsvm)\n",
    "logging.info(\"Balanced Accuracy: %f\", balanced_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80340a9",
   "metadata": {},
   "source": [
    "### OCSVM AUC-Kurven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_scores_ocsvm = -ocsvm.decision_function(val_errors.reshape(-1,1))\n",
    "y_val_seq_ocsvm = np.array([y_val_full[i + seq_length - 1] for i in range(len(val_scores_ocsvm))])\n",
    "true_labels_ocsvm = y_val_seq_ocsvm.astype(int)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(true_labels_ocsvm, val_scores_ocsvm)\n",
    "roc_auc = roc_auc_score(true_labels_ocsvm, val_scores_ocsvm)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(true_labels_ocsvm, val_scores_ocsvm)\n",
    "pr_auc = average_precision_score(true_labels_ocsvm, val_scores_ocsvm)\n",
    "# Plotten\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ROC OCSVM\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('roc_curve_ocsvm.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# PR OCSVM\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f\"AP = {pr_auc:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('pr_curve_ocsvm.png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbec00d7",
   "metadata": {},
   "source": [
    "# Modell 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99cac7",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aea0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps = 0.05, min_samples = 40)\n",
    "\n",
    "dbscan_labels = dbscan.fit_predict(test_errors.reshape(-1, 1))\n",
    "dbscan_anomaly = (dbscan_labels == -1).astype(int)\n",
    "\n",
    "y_test_seq_dbscan = np.array([y_test_full[i + seq_length - 1] for i in range(len(dbscan_anomaly))])\n",
    "true_labels_dbscan = y_test_seq_dbscan.astype(int)\n",
    "logging.info(\"\\n\" + classification_report(true_labels_dbscan, dbscan_anomaly))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab11cd46",
   "metadata": {},
   "source": [
    "### DBSCAN MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc = matthews_corrcoef(true_labels_dbscan, dbscan_anomaly)\n",
    "logging.info(\"Matthews Correlation Coefficient: %f\", mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5811f342",
   "metadata": {},
   "source": [
    "### DBSCAN Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53ffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_acc = balanced_accuracy_score(true_labels_dbscan, dbscan_anomaly)\n",
    "logging.info(\"Balanced Accuracy: %f\", balanced_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2569e8b7",
   "metadata": {},
   "source": [
    "### DBSCAN AUC-Kurven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc51f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = dbscan_anomaly\n",
    "\n",
    "fpr, tpr, _ = roc_curve(true_labels_dbscan, scores)\n",
    "roc_auc = roc_auc_score(true_labels_dbscan, scores)\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(true_labels_dbscan, scores)\n",
    "pr_auc = average_precision_score(true_labels_dbscan, scores)\n",
    "plt.figure(figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd99071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC-Kurve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve (DBSCAN)\")\n",
    "plt.legend()\n",
    "plt.savefig('roc_curve_dbscan.png', dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR-Kurve\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(recall, precision, label=f\"AP = {pr_auc:.3f}\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision-Recall Curve (DBSCAN)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('pr_curve_dbscan.png', dpi=300)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
